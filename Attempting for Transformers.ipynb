{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2790a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message= 'Series.__getitem__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f26c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fca4301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            int64\n",
       "0                    object\n",
       "1                    object\n",
       "2                    object\n",
       "source               object\n",
       "hash                 object\n",
       "topic1              float64\n",
       "topic2              float64\n",
       "topic3              float64\n",
       "topic4              float64\n",
       "topic5              float64\n",
       "topic6              float64\n",
       "topic7              float64\n",
       "topic8              float64\n",
       "topic9              float64\n",
       "topic10             float64\n",
       "topic11             float64\n",
       "topic12             float64\n",
       "topic13             float64\n",
       "topic14             float64\n",
       "topic15             float64\n",
       "best topic           object\n",
       "best probability    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset_topics_60k.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8f0043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"visiting the graves stronger the october wind at my grandparents'\",\n",
       " 'profound blue of night  the resin and salt of pines so far from the sea',\n",
       " 'scattered in the ditch  like tiny scraps of blue sky bits of plastic bag',\n",
       " 'the smell of her hands on the neck of the bottle drinking greedily',\n",
       " \"christmas services a cellular phone rings out handel's messiah\",\n",
       " \"gazing at the moon on a still summer's evening feast for mosquitoes\",\n",
       " 'my tea gets colder and the madeleine just sinks memory betrays',\n",
       " 'small green waves crashing against a porcelain rim morning tea tempest',\n",
       " 'red poppies growing between rows of white tombstones as in remembrance',\n",
       " 'in front of bronze doors they huddle against the cold the newly homeless']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to create a string instance based on specific columns in a DataFrame\n",
    "def combine_columns_to_string(index):\n",
    "    # Ensuring the index is non-negative\n",
    "    assert index >= 0, 'Index cannot be a negative integer'\n",
    "\n",
    "    # Retrieving the row at the specified index\n",
    "    selected_row = df.iloc[index, :]\n",
    "\n",
    "    # Combining columns 0, 1, and 2 into a single string\n",
    "    # making the poem seem together at similar manner\n",
    "    combined_string = str(selected_row[1]) +' ' +str(selected_row[2]) + ' '+str(selected_row[3])\n",
    "\n",
    "    return combined_string\n",
    "\n",
    "# Applying the function to each row in the df\n",
    "document = [combine_columns_to_string(i) for i in range(len(df.iloc[:, 2]))]\n",
    "print(len(document))\n",
    "document[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e0fcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visiting the graves stronger the october wind at my grandparents',\n",
       " 'profound blue of night  the resin and salt of pines so far from the sea',\n",
       " 'scattered in the ditch  like tiny scraps of blue sky bits of plastic bag',\n",
       " 'the smell of her hands on the neck of the bottle drinking greedily',\n",
       " 'christmas services a cellular phone rings out handels messiah']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning the document\n",
    "document = [string.replace('\\'', '') for string in document]\n",
    "document[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34600c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123455"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('cmudict')\n",
    "\n",
    "cmu_dict =nltk.corpus.cmudict.dict()\n",
    "words_not_in_cmu = []\n",
    "len(cmu_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db9eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_syllables_word(word):\n",
    "    if word == '<mask>':\n",
    "        return -1\n",
    "    try:\n",
    "        # from: https://datascience.stackexchange.com/questions/23376/how-to-get-the-number-of-syllables-in-a-word\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in cmu_dict[word.lower()]][0]\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "def n_syllables(sentence):\n",
    "    num_syllabes = 0\n",
    "    for word in sentence.split():\n",
    "        num_s = num_syllables_word(word)\n",
    "        if num_s != -1:\n",
    "            num_syllabes += num_syllables_word(word)\n",
    "    return num_syllabes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593a3ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_syllables('I am a deep learning student'), n_syllables('student')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e0a4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.str_"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting all the topics for each document poem\n",
    "all_topics = df['best topic'].values.astype(str)\n",
    "type(all_topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7eb0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arranging the masks randomly for each words and embedding the syllable count for each stanza?\n",
    "def random_masking(line):\n",
    "    \n",
    "    indv_words = line.split()\n",
    "    for i in range(len(indv_words)):\n",
    "        rand = np.random.uniform(0, 1)\n",
    "        \n",
    "        if rand>0.6:\n",
    "            indv_words[i] = '<*>'\n",
    "    line = ' '.join(indv_words)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c6ecfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There <*> many <*> in the sack'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_masking('There are many potatoes in the sack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ce1ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_entire_line(full_line):\n",
    "    masked_line = []\n",
    "    masked_part_line = ''\n",
    "    \n",
    "    for part_line in full_line:\n",
    "        masked_part_line = random_masking(part_line)\n",
    "        masked_line.append(masked_part_line)\n",
    "        \n",
    "    return masked_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "928c078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<*> <*> go <*> <*> along',\n",
       " 'all <*> way into <*> abyss',\n",
       " 'and keep it <*> that <*> <*> <*> best']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#every time, the masking is different to make it random while training\n",
    "mask_entire_line(['Here we go now far along', 'all the way into the abyss', 'and keep it common that we are the best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88467d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start and end token\n",
    "ST = '<s>'#Start Token\n",
    "ET = '</s>'#End Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6813ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating masked poems and original poems for the comparison\n",
    "# this one just to see what really happens in it\n",
    "def mask_df_demo(df):\n",
    "    masked_poems = []\n",
    "    org_poems = []\n",
    "    \n",
    "    for i, comb in enumerate(df.iterrows()):\n",
    "        index, row = comb\n",
    "        poem_topic = all_topics[i]\n",
    "        \n",
    "        mask_1 = random_masking(row[1])\n",
    "        mask_2 = random_masking(row[2])\n",
    "        mask_3 = random_masking(row[3])\n",
    "        \n",
    "        syllable_1 = str(n_syllables(row[1]))\n",
    "        syllable_2 = str(n_syllables(row[2]))\n",
    "        syllable_3 = str(n_syllables(row[3]))\n",
    "        \n",
    "        masked_poem_1 = ' '.join([poem_topic, ST, mask_1, syllable_1, ET])\n",
    "        masked_poem_2 = ' '.join([ST, mask_2, syllable_2, ET])\n",
    "        masked_poem_3 = ' '.join([ST, mask_3, syllable_3, ET])\n",
    "        \n",
    "        org_poem_1 = ' '.join([poem_topic, ST, row[1], syllable_1, ET])\n",
    "        org_poem_2 = ' '.join([ST, row[2], syllable_2, ET])\n",
    "        org_poem_3 = ' '.join([ST, row[3], syllable_3, ET])\n",
    "        \n",
    "        overall_masked_poem = ' '.join([masked_poem_1,masked_poem_2, masked_poem_3])\n",
    "        overall_org_poem = ' '.join([org_poem_1,org_poem_2, org_poem_3])\n",
    "        print(overall_masked_poem)\n",
    "        print(overall_org_poem)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b9031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dde17984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic12 <s> <*> the graves 5 </s> <s> <*> the october wind 7 </s> <s> at <*> <*> 5 </s>\n",
      "topic12 <s> visiting the graves 5 </s> <s> stronger the october wind 7 </s> <s> at my grandparents' 5 </s>\n"
     ]
    }
   ],
   "source": [
    "mask_df_demo(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1a783eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating masked poems and original poems for the comparison\n",
    "def mask_df(df):\n",
    "    masked_poems = []\n",
    "    org_poems = []\n",
    "    \n",
    "    for i, comb in enumerate(df.iterrows()):\n",
    "        index, row = comb\n",
    "        poem_topic = all_topics[i]\n",
    "        \n",
    "        mask_1 = random_masking(row[1])\n",
    "        mask_2 = random_masking(row[2])\n",
    "        mask_3 = random_masking(row[3])\n",
    "        \n",
    "        syllable_1 = str(n_syllables(row[1]))\n",
    "        syllable_2 = str(n_syllables(row[2]))\n",
    "        syllable_3 = str(n_syllables(row[3]))\n",
    "        \n",
    "        masked_poem_1 = ' '.join([poem_topic, ST, mask_1, syllable_1, ET])\n",
    "        masked_poem_2 = ' '.join([ST, mask_2, syllable_2, ET])\n",
    "        masked_poem_3 = ' '.join([ST, mask_3, syllable_3, ET])\n",
    "        \n",
    "        org_poem_1 = ' '.join([poem_topic, ST, row[1], syllable_1, ET])\n",
    "        org_poem_2 = ' '.join([ST, row[2], syllable_2, ET])\n",
    "        org_poem_3 = ' '.join([ST, row[3], syllable_3, ET])\n",
    "        \n",
    "        overall_masked_poem = ' '.join([masked_poem_1,masked_poem_2, masked_poem_3])\n",
    "        overall_org_poem = ' '.join([org_poem_1,org_poem_2, org_poem_3])\n",
    "        \n",
    "        masked_poems.append(overall_masked_poem)\n",
    "        org_poems.append(overall_org_poem)\n",
    "        \n",
    "        \n",
    "    assert(len(masked_poems)==len(org_poems))\n",
    "    return masked_poems, org_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "532a95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = mask_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6803111a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106,\n",
       " 106,\n",
       " \"topic12 <s> visiting the graves 5 </s> <s> stronger <*> october <*> 7 </s> <s> at <*> grandparents' 5 </s>\",\n",
       " \"topic12 <s> visiting the graves 5 </s> <s> stronger the october wind 7 </s> <s> at my grandparents' 5 </s>\")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "len(X[0]), len(Y[0]), X[0], Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cf33dca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26327, 26327)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "00de92bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"topic12 <s> visiting the graves 5 </s> <s> stronger <*> october <*> 7 </s> <s> at <*> grandparents' 5 </s>\",\n",
       " \"topic12 <s> visiting the graves 5 </s> <s> stronger the october wind 7 </s> <s> at my grandparents' 5 </s>\")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c70c6fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"topic12 <s> visiting the graves 5 </s> <s> stronger <*> october <*> 7 </s> <s> at <*> grandparents' 5 </s>\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "85d35083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 106)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0]), len(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "24b4ae23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21061, 5266)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n",
    "len(X_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ec10826f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('topic3 <s> dreams seldom <*> 5 </s> <s> when <*> <*> so 7 </s> <s> <*> <*> <*> 5 </s>',\n",
       " 'topic3 <s> dreams seldom linger 5 </s> <s>  when reality is so 7 </s> <s> eager to resume 5 </s>')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35357f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5bedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e6fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6eb4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad85d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9a2e1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AlbertTokenizer, AlbertModel, AlbertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5add145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7462a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c4a87a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8dc506ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_HUB_DISABLE_SYMLINKS_WARNING = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "207e3c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')#roBERTa tokenizer\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')#roBERTa model which is pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "31d7143d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a78373ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 96)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0]), len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b446a883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 41]) torch.Size([1, 32])\n",
      "CPU times: total: 11.5 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_tokenized = [tokenizer(xtr, return_tensors=\"pt\", padding=True) for xtr in X_train]\n",
    "X_test_tokenized = [tokenizer(xte, return_tensors=\"pt\", padding=True) for xte in X_test]\n",
    "y_train_tokenized = [tokenizer(ytr, return_tensors=\"pt\", padding=True) for ytr in y_train]\n",
    "y_test_tokenized = [tokenizer(yte, return_tensors=\"pt\", padding=True) for yte in y_test]\n",
    "print(X_train_tokenized[0][\"input_ids\"].shape, y_train_tokenized[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0bd50125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 41]), torch.Size([1, 32]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized[0]['input_ids'].shape, y_train_tokenized[0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "df25fc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "08f40127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a41424b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3ce0324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertModel(\n",
       "  (embeddings): AlbertEmbeddings(\n",
       "    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (encoder): AlbertTransformer(\n",
       "    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (albert_layer_groups): ModuleList(\n",
       "      (0): AlbertLayerGroup(\n",
       "        (albert_layers): ModuleList(\n",
       "          (0): AlbertLayer(\n",
       "            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (attention): AlbertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attention_dropout): Dropout(p=0, inplace=False)\n",
       "              (output_dropout): Dropout(p=0, inplace=False)\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pooler_activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e2ad8caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 45260,   246,  1437,     0, 28696,  3226, 15698, 28696,  3226,\n",
      "         15698,   195,  1437,     2,  1437,     0, 28696,  3226, 15698,    51,\n",
      "            95,  3008,     8,     5,   262,  1437,     2,  1437,     0,   232,\n",
      "         28696,  3226, 15698, 28696,  3226, 15698,  1431,   195,  1437,     2,\n",
      "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} {'input_ids': tensor([[    0, 45260,   246,  1437,     0,  4739,  7154,   195,  1437,     2,\n",
      "          1437,     0,  1437,  1067,    51,    95,  3008,     8,     5,   262,\n",
      "          1437,     2,  1437,     0,   232,   198,   106,  1431,   195,  1437,\n",
      "             2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21061, 5266)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train_tokenized[0], y_train_tokenized[0])\n",
    "len(X_train_tokenized), len(X_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a277eee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000023C557F2EA0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d8d3724c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0004\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 4e-4)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7454251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "15e819e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63183, 15798)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_steps = epochs * len(X_train)\n",
    "test_steps = epochs * len(X_test)\n",
    "training_steps, test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7fd10bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_scheduler in module transformers.optimization:\n",
      "\n",
      "get_scheduler(name: Union[str, transformers.trainer_utils.SchedulerType], optimizer: torch.optim.optimizer.Optimizer, num_warmup_steps: Optional[int] = None, num_training_steps: Optional[int] = None)\n",
      "    Unified API to get any scheduler from its name.\n",
      "    \n",
      "    Args:\n",
      "        name (`str` or `SchedulerType`):\n",
      "            The name of the scheduler to use.\n",
      "        optimizer (`torch.optim.Optimizer`):\n",
      "            The optimizer that will be used during training.\n",
      "        num_warmup_steps (`int`, *optional*):\n",
      "            The number of warmup steps to do. This is not required by all schedulers (hence the argument being\n",
      "            optional), the function will raise an error if it's unset and the scheduler type requires it.\n",
      "        num_training_steps (`int``, *optional*):\n",
      "            The number of training steps to do. This is not required by all schedulers (hence the argument being\n",
      "            optional), the function will raise an error if it's unset and the scheduler type requires it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(transformers.get_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6115090a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x23c389a6fd0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler = transformers.get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=training_steps\n",
    ")\n",
    "lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f7daaf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 45]), torch.Size([1, 34]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized[3][\"input_ids\"].shape, y_train_tokenized[3][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5b8f5356",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (45) to match target batch_size (34).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_tokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fastaging\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fastaging\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fastaging\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1103\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(prediction_scores\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1102\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1103\u001b[0m     masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1106\u001b[0m     output \u001b[38;5;241m=\u001b[39m (prediction_scores,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fastaging\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fastaging\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fastaging\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fastaging\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (45) to match target batch_size (34)."
     ]
    }
   ],
   "source": [
    "model(X_train_tokenized[3][\"input_ids\"].to(device), labels=y_train_tokenized[3][\"input_ids\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d7c86489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train started\n",
      "tensor([[    0, 45260,   176,  1437,     0, 28696,  3226, 15698,   364, 10054,\n",
      "          5229,    18,   195,  1437,     2,  1437,     0,  7855,    16,   460,\n",
      "            15, 23618,  1951,   262,  1437,     2,  1437,     0,   313,   579,\n",
      "          1021,     7,    69,   195,  1437,     2,     2]], device='cuda:0') tensor([[    0, 45260,   176,  1437,     0,  2649,   219,   364, 10054,  5229,\n",
      "            18,   195,  1437,     2,  1437,     0,  1437,  7855,    16,   460,\n",
      "            15, 23618,  1951,   262,  1437,     2,  1437,     0,   313,   579,\n",
      "          1021,     7,    69,   195,  1437,     2,     2]], device='cuda:0')\n",
      "MaskedLMOutput(loss=None, logits=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_outputs)\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m model_outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# manipulating steps and then resetting gradients with zero_grad at the end\u001b[39;00m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "# logger for train and validation loss\n",
    "training_loss_logger = []\n",
    "test_loss_logger = []\n",
    "\n",
    "train_3k_losses = []\n",
    "test_3k_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    #torch.cuda.empty_cache()\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_done = 0\n",
    "    print('Train started')\n",
    "    for i in range(len(X_train_tokenized[:6001])):\n",
    "        if X_train_tokenized[i][\"input_ids\"].shape[1] == y_train_tokenized[i][\"input_ids\"].shape[1]:\n",
    "            \n",
    "            ## training the dataset taking X[i] and Y[i] of training sets\n",
    "            inp = X_train_tokenized[i][\"input_ids\"].to(device)\n",
    "            label = y_train_tokenized[i][\"input_ids\"].to(device)\n",
    "            print(inp, label)\n",
    "            model_outputs = model(inp, label)\n",
    "            print(model_outputs)\n",
    "            loss = model_outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # manipulating steps and then resetting gradients with zero_grad at the end\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #managing the loss back to appropriate format\n",
    "            loss = loss.cpu().detach().numpy()\n",
    "            \n",
    "            train_loss = train_loss + loss\n",
    "            train_done = train_done + 1\n",
    "            break\n",
    "            if i%3000==0:\n",
    "                train_3k_losses.append([i, train_loss/train_done])\n",
    "                print(f'Epoch: {epoch}, Train: {i} / {len(X_train_tokenized)}, loss = {train_loss/train_done}, Remaining: {len(X_train_tokenized)-i}')\n",
    "    \n",
    "    \n",
    "    train_skipped = len(X_test_tokenized) - train_done\n",
    "    train_loss = train_loss/train_done  \n",
    "    print(f\"epoch: {epoch+1}, train loss: {train_loss}, Skipped = {train_skipped}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_done = 0\n",
    "    for i in range(len(X_test_tokenized)):\n",
    "        if X_test_tokenized[i][\"input_ids\"].shape[1] == y_test_tokenized[i][\"input_ids\"].shape[1]:\n",
    "            \n",
    "            ## testing the dataset taking X[i] and y[i] of test datasets\n",
    "            inp = X_test_tokenized[i][\"input_ids\"].to(device)\n",
    "            label = y_test_tokenized[i][\"input_ids\"].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inp, labels = label)\n",
    "                \n",
    "            loss = outputs.loss\n",
    "            \n",
    "            #managing the format of loss for alignment\n",
    "            loss = loss.cpu().detach().numpy()\n",
    "            \n",
    "            test_loss = validate_loss + loss\n",
    "            test_done = validate_done+1\n",
    "            break\n",
    "            if i%3000==0:\n",
    "                test_3k_losses.append([i, test_loss/test_done])\n",
    "                print(f'Epoch: {epoch}, Test: {i} / {len(X_test_tokenized)}, loss = {test_loss/test_done}, Remaining: {len(X_test_tokenized)-i}')\n",
    "            \n",
    "            \n",
    "    test_skipped = len(X_test_tokenized) - test_done\n",
    "    test_loss = test_loss/test_done  \n",
    "    \n",
    "    print(f\"epoch: {epoch+1}, validate loss: {test_loss}\")\n",
    "    training_loss_logger.append(train_loss)\n",
    "    validation_loss_logger.append(test_loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e890d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f4b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
